{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-wells",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I will be working on a dataset gotten from https://datasets.simula.no/depresjon/#download. The data was harvested from smart actigraph wrist watch belonging to 55 different individuals. This actigraph wristwatch takes patients health details such as patients motor activity, sleep/inactivity, heart rate, etc. In this case, the data contains the following columns:\n",
    "- timestamp (one minute intervals)\n",
    "- date (date of measurement)\n",
    "- activity (activity measurement from the actigraph watch). \n",
    "\n",
    "The data set is already separated into two folders, both containing actigraph data (csv files) of patients collected over time. The first group is stored in a folder called \"condition\" which contains actigraph data of 23 depressed patients who suffer from either bipolar/unipolar while the second folder \"control\" contains actigraph data of 32 mormal people with no signs of depression.\n",
    "\n",
    "There is another csv file containing the following columns; \n",
    "- number (patient identifier), \n",
    "- days (number of days of measurements), \n",
    "- gender (1 or 2 for female or male), \n",
    "- age (age in age groups), \n",
    "- afftype (1: bipolar II, 2: unipolar depressive, 3: bipolar I), \n",
    "- melanch (1: melancholia, 2: no melancholia), \n",
    "- inpatient (1: inpatient, 2: outpatient), \n",
    "- edu (education grouped in years), \n",
    "- marriage (1: married or cohabiting, 2: single), \n",
    "- work (1: working or studying, 2: unemployed/sick leave/pension), \n",
    "- madrs1 (MADRS score when measurement started), madrs2 (MADRS when measurement stopped). \n",
    "\n",
    "The essence of this work is to mix both data together and see if it will be possible to separate the mixed data back into their correct category using binary classification.\n",
    "\n",
    "Let us begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-community",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suffering-relevance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.538042Z",
     "start_time": "2021-04-16T12:11:55.800967Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06854d009826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-pillow",
   "metadata": {},
   "source": [
    "# Data Preparation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-sixth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.540812Z",
     "start_time": "2021-04-16T12:11:55.848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(folder, start = 0, stop = 1440, label = 1):\n",
    "    \"\"\"This function takes three arguments which are the folder to be fetched \n",
    "    from, the duration/number of minutes to observe. Default is 1440 \n",
    "    as 1440 minutes make one day and then the label of the observation. \n",
    "    The main idea is to use a numpy array to map the activity levels to a category\n",
    "    We are just saying... People who have their day like this (1440 minutes actigraph\n",
    "    data -that is the features X) are depressed or ok (That is the label y \n",
    "    which can be 1 or 0 but 1 by default depending on the folder we are examining).)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the file path using the folder name\n",
    "    folder = 'data/' + str(folder)\n",
    "    FileNames = os.listdir(folder)\n",
    "    \n",
    "    # initialise lists which will contain an array of the activities of each patient and their label.\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # iterate through each file in the above file path\n",
    "    for fileName in FileNames:\n",
    "        \n",
    "        # read each file\n",
    "        df = pd.read_csv(folder + '/' + str(fileName))\n",
    "        \n",
    "        # get all unique dates \n",
    "        dates = df['date'].unique()\n",
    "        \n",
    "        # make a list container for activity levels\n",
    "        activityLevelsPerDay = []\n",
    "        \n",
    "        # for each unique date\n",
    "        for date in dates:\n",
    "            \n",
    "            # if the observations are up to one full day(1440 mins)\n",
    "            if len(df[df['date']==date]) == 1440:\n",
    "                \n",
    "                # fetch only activity column\n",
    "                temp = pd.DataFrame(df[df['date']==date]).drop(columns=['timestamp','date'])\n",
    "                \n",
    "                # store the activity column inside our list container\n",
    "                activityLevelsPerDay.append(temp)\n",
    "                \n",
    "        # convert the activity levels into a numpy array suitable for machine learning.        \n",
    "        for dailyActivityLevel in activityLevelsPerDay:\n",
    "            activityVector = np.array(dailyActivityLevel[\"activity\"], dtype='float32')\n",
    "            \n",
    "            # append this activity numpy array to the initial list and generate a \n",
    "            # corresponding label too.\n",
    "            if len(activityVector) == 1440:\n",
    "                X.append(activityVector[start : stop])\n",
    "                y.append(np.array(label, dtype='float32'))\n",
    "    \n",
    "    # return the array\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-blogger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.541573Z",
     "start_time": "2021-04-16T12:11:55.851Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_data = get_data(folder='condition', label=1)\n",
    "X_condition = condition_data[0]\n",
    "y_condition = condition_data[1]\n",
    "\n",
    "control_data = get_data(folder='control', label=0)\n",
    "X_control = control_data[0]\n",
    "y_control = control_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-collection",
   "metadata": {},
   "source": [
    "Let us look at the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-hollywood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.542250Z",
     "start_time": "2021-04-16T12:11:55.901Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Data of people with depressive condition are\", len(X_condition), \"rows \\\n",
    "while those of the normal people are\", len(X_control), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-portugal",
   "metadata": {},
   "source": [
    "# Data Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-chemical",
   "metadata": {},
   "source": [
    "I will select a sample from the condition data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-spanking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.542927Z",
     "start_time": "2021-04-16T12:11:55.995Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X_condition[0])\n",
    "plt.title('Condition Group Sample Daily Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "\n",
    "# save image to include in 2nd Paper\n",
    "plt.savefig('Condition Group Sample Daily Activity Time-Series.jpg') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-allen",
   "metadata": {},
   "source": [
    "Also select a sample from the control data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-acrobat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.543568Z",
     "start_time": "2021-04-16T12:11:56.043Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X_control[0])\n",
    "plt.title('Control Group Sample Daily Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.savefig('Control Group Sample Daily Activity Time-Series.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-freight",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "From my observation, at night (that is first half of the day, between 0 and 720 minutes) people with depression appear more active than normal people. Although this seems true using the visualisation from above, but the visualisation is only from one sample and it would be unfair to generalise. I am going to show more visualisations below to confirm my observation by getting the average sleep pattern of each class and ploting the same graph. Thereafter, I will plot both graphs on one plot for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-vegetarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.544188Z",
     "start_time": "2021-04-16T12:11:56.093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get average for condition\n",
    "condition_sum_vector = X_condition[0]\n",
    "for x in range(1, len(X_condition)):\n",
    "    condition_sum_vector += X_condition[x]\n",
    "condition_avg_vector = condition_sum_vector / len(X_condition)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(condition_avg_vector)\n",
    "plt.title('Condition Group Average Daily Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.savefig('Condition Group Average Daily Activity Time-Series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-header",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.544898Z",
     "start_time": "2021-04-16T12:11:56.097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get average for control\n",
    "control_sum_vector = X_control[0]\n",
    "for x in range(1, len(X_control)):\n",
    "    control_sum_vector += X_control[x]\n",
    "control_sum_vector = control_sum_vector / len(X_control)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(control_sum_vector)\n",
    "plt.title('Control Group Average Daily Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.savefig('Control Group Average Daily Activity Time-Series.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-wayne",
   "metadata": {},
   "source": [
    "Now let us superimpose both graphs and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-touch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.545558Z",
     "start_time": "2021-04-16T12:11:56.146Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(control_sum_vector, label='control')\n",
    "plt.plot(condition_avg_vector, label='condition')\n",
    "plt.title('Control vs Condition Average Daily Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.savefig('Control vs Condition Average Daily Activity Time-Series.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-croatia",
   "metadata": {},
   "source": [
    "From the above, it is possible to separate the data in the first 720 minutes, precisely between 350 and 600 minutes which happens to be in the night. I will zoom in my visualisation below to further examine the data at night time, that is in the first half (720 minutes) of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-image",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.546424Z",
     "start_time": "2021-04-16T12:11:56.196Z"
    }
   },
   "outputs": [],
   "source": [
    "# get data for the first 720 minutes only.\n",
    "# condition_night = get_data(folder='condition', stop=720)\n",
    "# X_condition_night = condition_night[0]\n",
    "# \n",
    "# control_night = get_data(folder='control', stop=720)\n",
    "# X_control_night = control_night[0]\n",
    "\n",
    "# get data for the first 720 minutes only.\n",
    "condition_night = get_data(folder='condition', start=300, stop=600)\n",
    "X_condition_night = condition_night[0]\n",
    "\n",
    "control_night = get_data(folder='control', start=300, stop=600)\n",
    "X_control_night = control_night[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-aircraft",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.547111Z",
     "start_time": "2021-04-16T12:11:56.200Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X_condition_night[0])\n",
    "plt.title('Condition Group Sample Night-Time Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.savefig('Condition Group Sample Night-Time Activity Time-Series.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-exercise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.547772Z",
     "start_time": "2021-04-16T12:11:56.204Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X_control_night[0])\n",
    "plt.title('Control Group Sample Night-Time Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.savefig('Control Group Sample Night-Time Activity Time-Series.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-grass",
   "metadata": {},
   "source": [
    "From the last two graphs above, viewing only the data at night time enables us to tell the difference more clearly. We will go ahead to find the average and plot again in like manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-omaha",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.548394Z",
     "start_time": "2021-04-16T12:11:56.254Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_sum_vector = X_condition_night[0]\n",
    "for x in range(1, len(X_condition_night)):\n",
    "    condition_sum_vector += X_condition_night[x]\n",
    "condition_avg_vector = condition_sum_vector / len(X_condition_night)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(condition_avg_vector)\n",
    "plt.title('Condition Group Average Night-Time Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.savefig('Condition Group Average Night-Time Activity Time-Series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-jimmy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.549075Z",
     "start_time": "2021-04-16T12:11:56.258Z"
    }
   },
   "outputs": [],
   "source": [
    "control_sum_vector = X_control_night[0]\n",
    "for x in range(1, len(X_control_night)):\n",
    "    control_sum_vector += X_control_night[x]\n",
    "control_sum_vector = control_sum_vector / len(X_control_night)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(control_sum_vector)\n",
    "plt.title('Control Group Average Night-Time Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.savefig('Control Group Average Night-Time Activity Time-Series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-decade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.549631Z",
     "start_time": "2021-04-16T12:11:56.262Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(control_sum_vector, label='control')\n",
    "plt.plot(condition_avg_vector, label='condition')\n",
    "plt.title('Control vs Condition Average Night-Time Activity Time-Series')\n",
    "plt.ylabel('Activity Level')\n",
    "plt.xlabel('Minute of the day')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.savefig('Control vs Condition Average Night-Time Activity Time-Series.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-cabinet",
   "metadata": {},
   "source": [
    "So working with data collected only between 300 and 600 minutes might do a better job in classifying our model correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-george",
   "metadata": {},
   "source": [
    "Now, let us combine our data and see if we can truly separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-penny",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.550317Z",
     "start_time": "2021-04-16T12:11:56.363Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "condition = get_data(folder='condition', label=1)\n",
    "X_condition = condition[0]\n",
    "y_condition = condition[1]\n",
    "\n",
    "\n",
    "control = get_data(folder='control', label=0)\n",
    "X_control = control[0]\n",
    "y_control = control[1]\n",
    "\n",
    "\n",
    "X_condition.extend(X_control)\n",
    "X = X_condition\n",
    "\n",
    "\n",
    "y_condition.extend(y_control)\n",
    "y = y_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-rabbit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.551047Z",
     "start_time": "2021-04-16T12:11:56.366Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(X[0]))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-peace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.551659Z",
     "start_time": "2021-04-16T12:11:56.371Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-macedonia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.552316Z",
     "start_time": "2021-04-16T12:11:56.374Z"
    }
   },
   "outputs": [],
   "source": [
    "combinedDict = list(zip(X, y))\n",
    "random.shuffle(combinedDict)\n",
    "X[:], y[:] = zip(*combinedDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-congress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T11:53:31.237314Z",
     "start_time": "2021-04-09T11:53:30.411838Z"
    }
   },
   "source": [
    "### combinedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-increase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T15:07:02.512678Z",
     "start_time": "2021-04-06T15:07:02.298725Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-blast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.552957Z",
     "start_time": "2021-04-16T12:11:56.430Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y, dtype='float32')\n",
    "# X = np.reshape(X, (X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-borough",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.553621Z",
     "start_time": "2021-04-16T12:11:56.433Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-nursery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.554261Z",
     "start_time": "2021-04-16T12:11:56.437Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "kfold = StratifiedKFold(n_splits=20, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-prescription",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.554870Z",
     "start_time": "2021-04-16T12:11:56.440Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.unique(y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-horse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T14:21:57.584909Z",
     "start_time": "2021-04-08T14:21:57.543711Z"
    }
   },
   "source": [
    "I used the function bellow to perform a grid search when choosing the best C and gamma parameters for my SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-moldova",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.555560Z",
     "start_time": "2021-04-16T12:11:56.494Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## C = [2**(1 + (n/100)) for n in range(-1025, 1025, 25)]\n",
    "# C = np.arange(0.79, 0.82, 0.0001)\n",
    "# bestC = {}\n",
    "# i = 0\n",
    "# l = len(C)\n",
    "# accuracy = []\n",
    "# for c in C:\n",
    "#     i += 1\n",
    "#     SupportVectorClassModel = SVC(kernel='rbf', C=c)\n",
    "#     for train, test in kfold.split(X, y):\n",
    "#         SupportVectorClassModel.fit(X[train], y[train])\n",
    "#         y_pred = SupportVectorClassModel.predict(X[test])\n",
    "#         acc = accuracy_score(y[test], y_pred)*100\n",
    "#         accuracy.append(acc)\n",
    "#         progress = str(round(((i * 100)/(l)), 1)) + \"%\"\n",
    "#         \n",
    "#     if i % 10 == 0:\n",
    "#         print(\"Accuracy = %-20s C = %-20s progress = %-20s\" % (acc, c, progress))\n",
    "# \n",
    "#     if acc > 73:\n",
    "#         bestC[acc] = c\n",
    "#         cm = confusion_matrix(y[test], y_pred)\n",
    "#         print(\"-----------------------------------Accuracy =\", acc, \"\\nC =\", c)\n",
    "#         print(cm)\n",
    "# \n",
    "# \n",
    "# \n",
    "# print(\"Maximum accuracy achieved is\", max(bestC, key=int), \"with c =\", bestC[max(bestC, key=int)])\n",
    "# print(\"Training samples:\", len(X[train]), \"\\nwhile test samples:\", len(X[test]))\n",
    "# acc = round((sum(accuracy)/len(accuracy)), 4)\n",
    "# print(acc)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-stack",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.556161Z",
     "start_time": "2021-04-16T12:11:56.499Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "SupportVectorClassModel = SVC(kernel='rbf', C=0.8)\n",
    "for train, test in kfold.split(X, y):\n",
    "    SupportVectorClassModel.fit(X[train], y[train])\n",
    "    y_pred = SupportVectorClassModel.predict(X[test])\n",
    "    acc = accuracy_score(y[test], y_pred)*100\n",
    "    accuracy.append(acc)\n",
    "    print(\"Accuracy = %-20s\" % (acc))\n",
    "    cm = confusion_matrix(y[test], y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(y[test], y_pred))\n",
    "\n",
    "\n",
    "print(\"Training samples:\", len(X[train]), \"\\nwhile test samples:\", len(X[test]))\n",
    "acc = round((sum(accuracy)/len(accuracy)), 4)\n",
    "print(\"The average accuracy score after cross validation is\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-upgrade",
   "metadata": {},
   "source": [
    "Now, I want to get a similar code which I can apply to other algorithms as well and with single averaged metrics values instead of having them for each individual cross validation.\n",
    "\n",
    "Let us do that in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-longer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.556820Z",
     "start_time": "2021-04-16T12:11:56.554Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"This function returns a list of dictionaries containing the required evaluation metrics\"\"\"\n",
    "    result = []\n",
    "    label = [0, 1]\n",
    "    # the label created above represents category 1 or 0 and is used to get evaluation metrics for\n",
    "    # both classes individually. label 0 represents not_depressed, and 1 represents depressed.\n",
    "    \n",
    "    for pos_label in label:\n",
    "        metrics = {}\n",
    "        \n",
    "        # get confusion matrix prediction values\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # fetch the matrics\n",
    "        prec = precision_score(y_true, y_pred, pos_label=pos_label)\n",
    "        rec = recall_score(y_true, y_pred, pos_label=pos_label)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        spec = tn / (tn + fp)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, labels=None, pos_label=pos_label)\n",
    "        \n",
    "        # attach the metrics to dictionary\n",
    "        metrics['prec'] = round(prec, 3)\n",
    "        metrics['rec'] = round(rec, 3)\n",
    "        metrics['acc'] = round(acc, 3)\n",
    "        metrics['spec'] = round(spec, 3)\n",
    "        metrics['mcc'] = round(mcc, 3)\n",
    "        metrics['f1'] = round(f1, 3)\n",
    "        \n",
    "        # put the dictionary into a list.\n",
    "        result.append(metrics)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_average(result, metric, n_splits, category=1):\n",
    "    \"\"\" 'result' is a list of dictionary which stores metrics derived accross several cross validations. When cross \n",
    "    validation of n_splits happens, this function returns the average of a paticular \n",
    "    'metric' appearing in 'result'. Specifying category = 1 or 0 gets the average \n",
    "    matrix for a particular category (depressed or non depressed)\"\"\"\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(n_splits):\n",
    "        sum += result[i][category][str(metric)]\n",
    "    return sum/(n_splits)\n",
    "\n",
    "\n",
    "def cross_validation_(algorithm, n_splits = 10):\n",
    "    \"\"\"This function does kfold validation on a model/algorithm, trains it and returns the overall weighted average\"\"\"\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits, shuffle=True, random_state=seed)\n",
    "    # kfold declaration\n",
    "    \n",
    "    metrics_container = []\n",
    "    # create a container to put in the resulting metrics from each crossvalidation\n",
    "    \n",
    "    # cross validation happens\n",
    "    for train, test in kfold.split(X, y):\n",
    "        \n",
    "        # train the model\n",
    "        algorithm.fit(X[train], y[train])\n",
    "        \n",
    "        # predict\n",
    "        y_pred = algorithm.predict(X[test])\n",
    "        \n",
    "        # get the metrics and put into the metrics_container\n",
    "        metric = get_metrics(y[test], y_pred)\n",
    "        metrics_container.append(metric)\n",
    "        \n",
    "        \n",
    "    combine_metric = {}\n",
    "\n",
    "    # fish out the average metrics of both classes 0 and 1 and get their averages too.\n",
    "    depressed_prec = get_average(result=metrics_container, metric='prec', n_splits=n_splits, category=1)\n",
    "    not_depressed_prec = get_average(result=metrics_container, metric='prec', n_splits=n_splits, category=0)\n",
    "    weighted_prec = (depressed_prec + not_depressed_prec)/2 \n",
    "    \n",
    "    depressed_rec = get_average(result=metrics_container, metric='rec', n_splits=n_splits, category=1)\n",
    "    not_depressed_rec = get_average(result=metrics_container, metric='rec', n_splits=n_splits, category=0)\n",
    "    weighted_rec = (depressed_rec + not_depressed_rec)/2\n",
    "\n",
    "    acc = get_average(result=metrics_container, metric='acc', n_splits=n_splits)\n",
    "    \n",
    "    \n",
    "    mcc = get_average(result=metrics_container, metric='mcc', n_splits=n_splits)\n",
    "    \n",
    "    depressed_f1 = get_average(result=metrics_container, metric='f1', n_splits=n_splits, category=1)\n",
    "    not_depressed_f1 = get_average(result=metrics_container, metric='f1', n_splits=n_splits, category=0)\n",
    "    weighted_f1 = (depressed_f1 + not_depressed_f1)/2\n",
    "    \n",
    "    # compile them into dictionaries\n",
    "    not_depressed = {'prec': round(not_depressed_prec, 3), 'rec': round(not_depressed_rec, 3), \n",
    "                     'acc': round(acc, 3), 'mcc': round(mcc, 3), 'f1': round(not_depressed_f1, 3)}\n",
    "    depressed = {'prec': round(depressed_prec, 3), 'rec': round(depressed_rec, 3), \n",
    "                 'acc': round(acc, 3), 'mcc': round(mcc, 3), 'f1': round(depressed_f1, 3)}\n",
    "    mean = {'prec': round(weighted_prec, 3), 'rec': round(weighted_rec, 3), \n",
    "            'acc': round(acc, 3), 'mcc': round(mcc, 3), 'f1': round(weighted_f1, 3)}\n",
    "    \n",
    "    # put the dictionary into another dictionary\n",
    "    combine_metric['depressed'] = depressed\n",
    "    combine_metric['not_depressed'] = not_depressed\n",
    "    combine_metric['mean'] = mean\n",
    "    \n",
    "    # print output as dataframe to display the results neatly\n",
    "    print(pd.DataFrame(combine_metric).T)\n",
    "    \n",
    "    print(\"================================================\")\n",
    "    cm = confusion_matrix(y[test], y_pred)\n",
    "    print(cm)\n",
    "#    print(classification_report(y[test], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-nickel",
   "metadata": {},
   "source": [
    "# Support Vector Model (kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-worry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.557426Z",
     "start_time": "2021-04-16T12:11:56.615Z"
    }
   },
   "outputs": [],
   "source": [
    "SupportVectorClassModel = SVC(kernel='rbf', C=0.8)\n",
    "cross_validation_(SupportVectorClassModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-couple",
   "metadata": {},
   "source": [
    "# Support Vector Model (kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-richmond",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.558117Z",
     "start_time": "2021-04-16T12:11:56.672Z"
    }
   },
   "outputs": [],
   "source": [
    "# tune parameters\n",
    "# List Hyperparameters that we want to tune.\n",
    "C = list(np.arange(0.001, 1, 0.01))\n",
    "\n",
    "kernel = ['linear']\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C=C, kernel=kernel)\n",
    "\n",
    "#Create new KNN object\n",
    "SupportVectorClassModel = SVC()\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(SupportVectorClassModel, hyperparameters, cv=10)\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X, y)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "# Best leaf_size: 1\n",
    "# Best p: 2\n",
    "# Best n_neighbors: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-bishop",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.558754Z",
     "start_time": "2021-04-16T12:11:56.677Z"
    }
   },
   "outputs": [],
   "source": [
    "C = list(np.arange(0.001, 1, 0.01))\n",
    "print(len(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-seller",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.559637Z",
     "start_time": "2021-04-16T12:11:56.680Z"
    }
   },
   "outputs": [],
   "source": [
    "SupportVectorClassModel = SVC(kernel='linear', C=0.011)\n",
    "cross_validation_(SupportVectorClassModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-island",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-productivity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.560299Z",
     "start_time": "2021-04-16T12:11:56.741Z"
    }
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier()\n",
    "cross_validation_(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-stewart",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T09:50:50.821842Z",
     "start_time": "2021-04-15T09:50:50.820201Z"
    }
   },
   "source": [
    "## Can these scores be made better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-patient",
   "metadata": {},
   "source": [
    "### Do not run the next code. it was only used for Grid Search in order to find the best parameters. It is computationally expensive  and takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-signature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.560962Z",
     "start_time": "2021-04-16T12:11:56.860Z"
    }
   },
   "outputs": [],
   "source": [
    "# # tune parameters\n",
    "# # List Hyperparameters that we want to tune.\n",
    "# leaf_size = list(range(1,50))\n",
    "# n_neighbors = list(range(1,30))\n",
    "# p=[1,2]\n",
    "# \n",
    "# #Convert to dictionary\n",
    "# hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "# \n",
    "# #Create new KNN object\n",
    "# knn = KNeighborsClassifier()\n",
    "# \n",
    "# #Use GridSearch\n",
    "# clf = GridSearchCV(knn, hyperparameters, cv=10)\n",
    "# \n",
    "# #Fit the model\n",
    "# best_model = clf.fit(X, y)\n",
    "# \n",
    "# # Print The value of best Hyperparameters\n",
    "# print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "# print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "# print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "# # Output:\n",
    "# # Best leaf_size: 1\n",
    "# # Best p: 2\n",
    "# # Best n_neighbors: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-angel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.561607Z",
     "start_time": "2021-04-16T12:11:56.864Z"
    }
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=4, leaf_size=1, p=2)\n",
    "cross_validation_(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-canyon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:51:44.322246Z",
     "start_time": "2021-04-16T10:51:44.314325Z"
    }
   },
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-manitoba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.562366Z",
     "start_time": "2021-04-16T12:11:56.925Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "cross_validation_(gpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-london",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-murray",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.563300Z",
     "start_time": "2021-04-16T12:11:56.988Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 0)\n",
    "cross_validation_(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-batman",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-uncertainty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.563960Z",
     "start_time": "2021-04-16T12:11:57.050Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "cross_validation_(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-sherman",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-expansion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.564584Z",
     "start_time": "2021-04-16T12:11:57.113Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "cross_validation_(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-joining",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-criminal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.565181Z",
     "start_time": "2021-04-16T12:11:57.177Z"
    }
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "cross_validation_(gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-calculator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.565795Z",
     "start_time": "2021-04-16T12:11:57.181Z"
    }
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB(var_smoothing=1e-09)\n",
    "cross_validation_(gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-location",
   "metadata": {},
   "source": [
    "## Linear and Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-extra",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.566413Z",
     "start_time": "2021-04-16T12:11:57.246Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = QuadraticDiscriminantAnalysis()\n",
    "cross_validation_(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-cream",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-position",
   "metadata": {},
   "source": [
    "To train with neural networks,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-criticism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T09:50:51.385680Z",
     "start_time": "2021-04-15T09:50:51.229179Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-expense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-setting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T10:58:30.762029Z",
     "start_time": "2021-04-16T10:58:26.515893Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-simon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-system",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.567043Z",
     "start_time": "2021-04-16T12:11:57.379Z"
    }
   },
   "outputs": [],
   "source": [
    "for train, test in kfold.split(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(1, 300), return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', recall_m, precision_m, f1_m])\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=10, batch_size=64, verbose=0)\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[4], scores[4]))\n",
    "    print(\"\\n\")\n",
    "    accuracy_scores.append(scores[1] * 100)\n",
    "    prec_scores.append(scores[2])\n",
    "    rec_scores.append(scores[3])\n",
    "    f1_scores.append(scores[4])\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec_scores), np.std(prec_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec_scores), np.std(rec_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-pattern",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.567659Z",
     "start_time": "2021-04-16T12:11:57.382Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "prec_scores = []\n",
    "rec_scores = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-baghdad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T12:11:57.568206Z",
     "start_time": "2021-04-16T12:11:57.385Z"
    }
   },
   "outputs": [],
   "source": [
    "for train, test in kfold.split(X, y):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, input_shape=(1, 300))))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', recall_m, precision_m, f1_m])\n",
    "    \n",
    "#print(X[train].dtype)\n",
    "#print(y[train].dtype)\n",
    "\n",
    "    model.fit(X[train], y[train], epochs=10, batch_size=64, verbose=0)\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[4], scores[4]))\n",
    "    print(\"\\n\")\n",
    "    accuracy_scores.append(scores[1] * 100)\n",
    "    prec_scores.append(scores[2])\n",
    "    rec_scores.append(scores[3])\n",
    "    f1_scores.append(scores[4])\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec_scores), np.std(prec_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec_scores), np.std(rec_scores)))\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
